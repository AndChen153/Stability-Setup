{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced MPPT Data Stitching with Stabilization Preprocessing\n",
    "\n",
    "This notebook combines multiple MPPT trial CSV files with advanced preprocessing that:\n",
    "1. Detects individual stabilization points for each pixel\n",
    "2. Removes unstabilized data while maintaining data continuity\n",
    "3. Uses NaN padding and shifting to create smooth, continuous time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set matplotlib style for better plots\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stabilization Detection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_mppt_stable(pce, time_min):\n",
    "    \"\"\"\n",
    "    Detect where MPPT data stabilizes for a single pixel.\n",
    "    \n",
    "    Args:\n",
    "        pce: PCE data array for one pixel\n",
    "        time_min: Time array in minutes\n",
    "        \n",
    "    Returns:\n",
    "        int: Index where stable region begins\n",
    "    \"\"\"\n",
    "    if len(pce) < 50:  # Need minimum data for analysis\n",
    "        return 0\n",
    "        \n",
    "    win_pts_smooth = min(11, len(pce))  # Ensure we don't exceed array size\n",
    "    if win_pts_smooth % 2 == 0:  # Must be odd\n",
    "        win_pts_smooth -= 1\n",
    "        \n",
    "    kernel = np.ones(win_pts_smooth, dtype=float) / win_pts_smooth\n",
    "    pce_smooth = np.convolve(pce, kernel, mode='same')\n",
    "\n",
    "    # Instantaneous slope (% per minute)\n",
    "    dpdt = np.gradient(pce_smooth, time_min)\n",
    "\n",
    "    # Threshold that means steady\n",
    "    window_minutes = 0.5  # How long it must stay flat\n",
    "    noise = np.std(pce_smooth[-min(50, len(pce_smooth)):])\n",
    "    slope_thresh = noise / window_minutes\n",
    "    mask = np.abs(dpdt) < slope_thresh\n",
    "\n",
    "    # Sample rate (points / minute) from the median spacing\n",
    "    if len(time_min) > 1:\n",
    "        Fs = 1.0 / np.median(np.diff(time_min))\n",
    "        win_pts = max(int(round(window_minutes * Fs)), 1)\n",
    "    else:\n",
    "        win_pts = 1\n",
    "\n",
    "    # Running sum of the True/False mask\n",
    "    if len(mask) >= win_pts:\n",
    "        run_sum = np.convolve(mask.astype(int),\n",
    "                            np.ones(win_pts, dtype=int),\n",
    "                            mode='valid')\n",
    "        # First index where *all* points in the window are True\n",
    "        stable_candidates = np.where(run_sum == win_pts)[0]\n",
    "        if stable_candidates.size:\n",
    "            return stable_candidates[0]\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pce_for_stabilization(voltage_data, current_data, cell_area=0.128):\n",
    "    \"\"\"\n",
    "    Calculate PCE for stabilization detection using the exact formula from calculations.py.\n",
    "    \n",
    "    Args:\n",
    "        voltage_data: Voltage array (8 pixels x N timepoints)\n",
    "        current_data: Current array in mA (8 pixels x N timepoints) \n",
    "        cell_area: Cell area in mmÂ²\n",
    "        \n",
    "    Returns:\n",
    "        np.array: PCE data (8 pixels x N timepoints)\n",
    "    \"\"\"\n",
    "    # Convert mA to A, then calculate power in W, then normalize by illumination area\n",
    "    # Formula: (V * I / 1000) / (0.1 * cell_area) * 100\n",
    "    pce_data = ((voltage_data * current_data / 1000) / (0.1 * cell_area)) * 100\n",
    "    return pce_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_individual_stabilization_points(voltage_data, current_data, time_data, metadata):\n",
    "    \"\"\"\n",
    "    Detect stabilization points for each individual pixel.\n",
    "    \n",
    "    Args:\n",
    "        voltage_data: Voltage array (N_timepoints x 8_pixels)\n",
    "        current_data: Current array (N_timepoints x 8_pixels) \n",
    "        time_data: Time array in seconds\n",
    "        metadata: Metadata dict containing cell area\n",
    "        \n",
    "    Returns:\n",
    "        dict: Stabilization information for each pixel\n",
    "    \"\"\"\n",
    "    # Get cell area\n",
    "    cell_area = float(metadata.get('Cell Area (mm^2)', 0.128))\n",
    "    \n",
    "    # Convert time to minutes\n",
    "    time_minutes = time_data / 60.0\n",
    "    \n",
    "    # Calculate PCE for each pixel\n",
    "    pce_data = calculate_pce_for_stabilization(voltage_data.T, current_data.T, cell_area)\n",
    "    \n",
    "    stabilization_info = {}\n",
    "    \n",
    "    # Detect stabilization for each pixel\n",
    "    for pixel_idx in range(8):\n",
    "        pixel_pce = pce_data[pixel_idx, :]\n",
    "        stable_idx = detect_mppt_stable(pixel_pce, time_minutes)\n",
    "        \n",
    "        stabilization_info[f'Pixel_{pixel_idx + 1}'] = {\n",
    "            'stable_index': stable_idx,\n",
    "            'stable_time_seconds': time_data[stable_idx] if stable_idx < len(time_data) else time_data[-1],\n",
    "            'stable_time_minutes': time_minutes[stable_idx] if stable_idx < len(time_minutes) else time_minutes[-1],\n",
    "            'data_points_removed': stable_idx,\n",
    "            'data_points_remaining': len(time_data) - stable_idx\n",
    "        }\n",
    "    \n",
    "    return stabilization_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Data Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_stabilization_preprocessing(data, headers, metadata, filename):\n",
    "    \"\"\"\n",
    "    Apply stabilization preprocessing to a single file's data.\n",
    "    \n",
    "    This function:\n",
    "    1. Detects individual stabilization points for each pixel\n",
    "    2. Trims data before stabilization points\n",
    "    3. Pads with NaN to maintain uniform array lengths\n",
    "    \n",
    "    Args:\n",
    "        data: Raw data array (N_timepoints x N_columns)\n",
    "        headers: Column headers\n",
    "        metadata: Metadata dict\n",
    "        filename: Source filename for reporting\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (processed_data, stabilization_info)\n",
    "    \"\"\"\n",
    "    print(f\"\\nApplying stabilization preprocessing to {os.path.basename(filename)}...\")\n",
    "    \n",
    "    # Extract time and pixel data\n",
    "    header_dict = {value: index for index, value in enumerate(headers)}\n",
    "    time_col = header_dict['Time']\n",
    "    time_data = data[:, time_col]\n",
    "    \n",
    "    # Extract voltage and current data for 8 pixels\n",
    "    voltage_data = np.zeros((len(data), 8))\n",
    "    current_data = np.zeros((len(data), 8))\n",
    "    \n",
    "    for i in range(8):\n",
    "        v_col = header_dict.get(f'Pixel_{i+1} V')\n",
    "        i_col = header_dict.get(f'Pixel_{i+1} mA')\n",
    "        if v_col is not None and i_col is not None:\n",
    "            voltage_data[:, i] = data[:, v_col]\n",
    "            current_data[:, i] = data[:, i_col]\n",
    "    \n",
    "    # Parse metadata from array format to dict\n",
    "    metadata_dict = {}\n",
    "    for i in range(len(metadata) - 1):  # Skip header row\n",
    "        if len(metadata[i]) >= 2:\n",
    "            key = str(metadata[i][0])\n",
    "            value = str(metadata[i][1]) \n",
    "            if key and value and value.lower() != 'none':\n",
    "                metadata_dict[key] = value\n",
    "    \n",
    "    # Detect stabilization points\n",
    "    stabilization_info = detect_individual_stabilization_points(\n",
    "        voltage_data, current_data, time_data, metadata_dict\n",
    "    )\n",
    "    \n",
    "    # Create processed data with individual pixel trimming\n",
    "    processed_data = data.copy()\n",
    "    \n",
    "    # Apply individual stabilization trimming with NaN padding\n",
    "    for i in range(8):\n",
    "        pixel_name = f'Pixel_{i+1}'\n",
    "        stable_idx = stabilization_info[pixel_name]['stable_index']\n",
    "        \n",
    "        v_col = header_dict.get(f'Pixel_{i+1} V')\n",
    "        i_col = header_dict.get(f'Pixel_{i+1} mA')\n",
    "        \n",
    "        if v_col is not None and i_col is not None and stable_idx > 0:\n",
    "            # Set pre-stabilization data to NaN\n",
    "            processed_data[:stable_idx, v_col] = np.nan\n",
    "            processed_data[:stable_idx, i_col] = np.nan\n",
    "    \n",
    "    # Report stabilization results\n",
    "    print(\"  Stabilization points detected:\")\n",
    "    for pixel_name, info in stabilization_info.items():\n",
    "        print(f\"    {pixel_name}: {info['stable_time_minutes']:.1f} min ({info['data_points_removed']} points removed)\")\n",
    "    \n",
    "    return processed_data, stabilization_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_nan_values_for_continuity(previous_data, current_data, headers):\n",
    "    \"\"\"\n",
    "    Shift NaN values to maintain data continuity between files.\n",
    "    \n",
    "    If the previous file ends with NaN for a pixel and the current file \n",
    "    starts with NaN for the same pixel, move the starting NaN values \n",
    "    to the end to create continuous valid data.\n",
    "    \n",
    "    Args:\n",
    "        previous_data: Data from previous file (N_points x N_cols)\n",
    "        current_data: Data from current file (M_points x N_cols)\n",
    "        headers: Column headers\n",
    "        \n",
    "    Returns:\n",
    "        np.array: Current data with NaN values shifted for continuity\n",
    "    \"\"\"\n",
    "    if previous_data is None:\n",
    "        return current_data\n",
    "    \n",
    "    header_dict = {value: index for index, value in enumerate(headers)}\n",
    "    modified_data = current_data.copy()\n",
    "    shifts_applied = 0\n",
    "    \n",
    "    # Check each pixel for NaN continuity\n",
    "    for i in range(8):\n",
    "        v_col = header_dict.get(f'Pixel_{i+1} V')\n",
    "        i_col = header_dict.get(f'Pixel_{i+1} mA')\n",
    "        \n",
    "        if v_col is not None and i_col is not None:\n",
    "            # Check if previous file ends with NaN and current starts with NaN\n",
    "            prev_v_ends_nan = np.isnan(previous_data[-1, v_col])\n",
    "            prev_i_ends_nan = np.isnan(previous_data[-1, i_col])\n",
    "            curr_v_starts_nan = np.isnan(current_data[0, v_col])\n",
    "            curr_i_starts_nan = np.isnan(current_data[0, i_col])\n",
    "            \n",
    "            if (prev_v_ends_nan and prev_i_ends_nan and \n",
    "                curr_v_starts_nan and curr_i_starts_nan):\n",
    "                \n",
    "                # Find how many NaN values at the start\n",
    "                nan_count = 0\n",
    "                for j in range(len(current_data)):\n",
    "                    if np.isnan(current_data[j, v_col]) and np.isnan(current_data[j, i_col]):\n",
    "                        nan_count += 1\n",
    "                    else:\n",
    "                        break\n",
    "                \n",
    "                if nan_count > 0:\n",
    "                    # Shift the data: move valid data to beginning, NaN to end\n",
    "                    valid_data_v = current_data[nan_count:, v_col]\n",
    "                    valid_data_i = current_data[nan_count:, i_col]\n",
    "                    \n",
    "                    # Create new arrays with valid data at start, NaN at end\n",
    "                    new_v_data = np.full(len(current_data), np.nan)\n",
    "                    new_i_data = np.full(len(current_data), np.nan)\n",
    "                    \n",
    "                    new_v_data[:len(valid_data_v)] = valid_data_v\n",
    "                    new_i_data[:len(valid_data_i)] = valid_data_i\n",
    "                    \n",
    "                    modified_data[:, v_col] = new_v_data\n",
    "                    modified_data[:, i_col] = new_i_data\n",
    "                    \n",
    "                    shifts_applied += 1\n",
    "    \n",
    "    if shifts_applied > 0:\n",
    "        print(f\"    Applied NaN shifting for {shifts_applied} pixels to maintain continuity\")\n",
    "    \n",
    "    return modified_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced File Discovery and Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_mppt_files(data_directory):\n",
    "    \"\"\"\n",
    "    Automatically discover and sort MPPT CSV files by timestamp.\n",
    "    \n",
    "    Args:\n",
    "        data_directory (str): Path to directory containing CSV files\n",
    "        \n",
    "    Returns:\n",
    "        list: Sorted list of file paths\n",
    "    \"\"\"\n",
    "    csv_files = glob.glob(os.path.join(data_directory, \"*.csv\"))\n",
    "    \n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(f\"No CSV files found in {data_directory}\")\n",
    "    \n",
    "    file_timestamps = []\n",
    "    for file_path in csv_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        timestamp_match = re.match(r'([A-Za-z]{3}-\\d{2}-\\d{4}_\\d{2}-\\d{2}-\\d{2})', filename)\n",
    "        if timestamp_match:\n",
    "            timestamp_str = timestamp_match.group(1)\n",
    "            try:\n",
    "                timestamp = datetime.strptime(timestamp_str, '%b-%d-%Y_%H-%M-%S')\n",
    "                file_timestamps.append((timestamp, file_path))\n",
    "            except ValueError:\n",
    "                print(f\"Warning: Could not parse timestamp from {filename}\")\n",
    "                file_timestamps.append((datetime.min, file_path))\n",
    "        else:\n",
    "            print(f\"Warning: Could not extract timestamp from {filename}\")\n",
    "            file_timestamps.append((datetime.min, file_path))\n",
    "    \n",
    "    file_timestamps.sort(key=lambda x: x[0])\n",
    "    sorted_files = [file_path for timestamp, file_path in file_timestamps]\n",
    "    \n",
    "    print(f\"Found {len(sorted_files)} CSV files:\")\n",
    "    for i, file_path in enumerate(sorted_files):\n",
    "        print(f\"  {i+1}. {os.path.basename(file_path)}\")\n",
    "    \n",
    "    return sorted_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_mppt_file(file_path):\n",
    "    \"\"\"\n",
    "    Load and apply stabilization preprocessing to a single MPPT CSV file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to CSV file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (metadata, headers, processed_data, stabilization_info)\n",
    "    \"\"\"\n",
    "    print(f\"\\nLoading {os.path.basename(file_path)}...\")\n",
    "    \n",
    "    # Load the file as string array\n",
    "    arr = np.loadtxt(file_path, delimiter=\",\", dtype=str)\n",
    "    \n",
    "    # Find the header row\n",
    "    header_row_idx = np.where(arr == \"Time\")[0]\n",
    "    if len(header_row_idx) == 0:\n",
    "        raise ValueError(f\"No 'Time' column found in {file_path}\")\n",
    "    \n",
    "    header_row = header_row_idx[0]\n",
    "    headers = arr[header_row, :]\n",
    "    metadata = arr[:header_row + 1, :]\n",
    "    data = arr[header_row + 1:, :].astype(float)\n",
    "    \n",
    "    print(f\"  - Raw data shape: {data.shape}\")\n",
    "    print(f\"  - Raw time range: {data[0, 0]:.1f} to {data[-1, 0]:.1f} seconds\")\n",
    "    \n",
    "    # Apply stabilization preprocessing\n",
    "    processed_data, stabilization_info = apply_stabilization_preprocessing(\n",
    "        data, headers, metadata, file_path\n",
    "    )\n",
    "    \n",
    "    return metadata, headers, processed_data, stabilization_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Stitching Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_mppt_files_with_stabilization(file_paths, output_filename=None):\n",
    "    \"\"\"\n",
    "    Stitch multiple MPPT CSV files with advanced stabilization preprocessing.\n",
    "    \n",
    "    Args:\n",
    "        file_paths (list): List of file paths to stitch\n",
    "        output_filename (str): Optional output filename\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (combined_metadata, headers, combined_data, all_stabilization_info)\n",
    "    \"\"\"\n",
    "    if not file_paths:\n",
    "        raise ValueError(\"No files provided\")\n",
    "    \n",
    "    print(f\"\\n=== Enhanced MPPT File Stitching with Stabilization Preprocessing ===\")\n",
    "    print(f\"Processing {len(file_paths)} files...\")\n",
    "    \n",
    "    combined_data_list = []\n",
    "    cumulative_time = 0.0\n",
    "    reference_metadata = None\n",
    "    reference_headers = None\n",
    "    all_stabilization_info = {}\n",
    "    previous_data = None\n",
    "    \n",
    "    for i, file_path in enumerate(file_paths):\n",
    "        # Load and preprocess file\n",
    "        metadata, headers, processed_data, stabilization_info = load_and_preprocess_mppt_file(file_path)\n",
    "        \n",
    "        # Store reference metadata and headers from first file\n",
    "        if i == 0:\n",
    "            reference_metadata = metadata\n",
    "            reference_headers = headers\n",
    "        else:\n",
    "            # Validate headers match\n",
    "            if not np.array_equal(headers, reference_headers):\n",
    "                print(f\"Warning: Headers in {os.path.basename(file_path)} don't match reference\")\n",
    "        \n",
    "        # Apply NaN shifting for continuity\n",
    "        if i > 0:\n",
    "            processed_data = shift_nan_values_for_continuity(previous_data, processed_data, headers)\n",
    "        \n",
    "        # Adjust time column for continuity\n",
    "        if i > 0:\n",
    "            processed_data[:, 0] += cumulative_time\n",
    "        \n",
    "        # Update cumulative time for next file\n",
    "        cumulative_time = processed_data[-1, 0]\n",
    "        \n",
    "        # Store data and info\n",
    "        combined_data_list.append(processed_data)\n",
    "        all_stabilization_info[os.path.basename(file_path)] = stabilization_info\n",
    "        previous_data = processed_data\n",
    "        \n",
    "        print(f\"  File {i+1}: {processed_data.shape[0]} data points, time range: {processed_data[0, 0]:.1f} to {processed_data[-1, 0]:.1f} s\")\n",
    "    \n",
    "    # Combine all data\n",
    "    combined_data = np.vstack(combined_data_list)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    total_points = combined_data.shape[0]\n",
    "    valid_data_points = np.sum(~np.isnan(combined_data[:, 1]))  # Count non-NaN voltage points for first pixel\n",
    "    nan_points = total_points - valid_data_points\n",
    "    \n",
    "    print(f\"\\n=== Combined Dataset Statistics ===\")\n",
    "    print(f\"  - Total data points: {total_points:,}\")\n",
    "    print(f\"  - Valid data points: {valid_data_points:,} ({valid_data_points/total_points*100:.1f}%)\")\n",
    "    print(f\"  - NaN data points: {nan_points:,} ({nan_points/total_points*100:.1f}%)\")\n",
    "    print(f\"  - Total time span: {combined_data[-1, 0]:.1f} seconds ({combined_data[-1, 0]/3600:.2f} hours)\")\n",
    "    print(f\"  - Columns: {len(reference_headers)}\")\n",
    "    \n",
    "    # Save combined file if requested\n",
    "    if output_filename:\n",
    "        save_combined_data_with_nan(reference_metadata, reference_headers, combined_data, output_filename)\n",
    "    \n",
    "    return reference_metadata, reference_headers, combined_data, all_stabilization_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_combined_data_with_nan(metadata, headers, data, output_filename):\n",
    "    \"\"\"\n",
    "    Save the combined data to a CSV file, properly handling NaN values.\n",
    "    \n",
    "    Args:\n",
    "        metadata (np.array): Metadata from original files\n",
    "        headers (np.array): Column headers\n",
    "        data (np.array): Combined numeric data with NaN values\n",
    "        output_filename (str): Output filename\n",
    "    \"\"\"\n",
    "    # Convert data to string format, preserving NaN as empty strings\n",
    "    data_str = np.empty_like(data, dtype=str)\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            if np.isnan(data[i, j]):\n",
    "                data_str[i, j] = ''  # Empty string for NaN\n",
    "            else:\n",
    "                data_str[i, j] = str(data[i, j])\n",
    "    \n",
    "    # Combine metadata and data\n",
    "    full_array = np.vstack([metadata, data_str])\n",
    "    \n",
    "    # Save to file\n",
    "    np.savetxt(output_filename, full_array, delimiter=',', fmt='%s')\n",
    "    \n",
    "    print(f\"\\nCombined data saved to: {output_filename}\")\n",
    "    print(f\"File size: {os.path.getsize(output_filename) / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Analysis and Visualization Functions"
   ]
  },
  {
   "
